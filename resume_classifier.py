# -*- coding: utf-8 -*-
"""resume_Classifier.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ebK-MvBvIeH1pXxwkdglNVIyl3CI4ws5
"""

#used to install a library called python-docx
!pip install python-docx PyPDF2

!sudo apt-get install antiword

!pip install spacy
!python -m spacy download en_core_web_sm

from google.colab import drive
drive.mount('/content/drive')

import zipfile
import os

zip_path = "/content/drive/MyDrive/resume_dataset.zip"
extract_path = "/content/drive/MyDrive/resume_dataset_unzipped"

# Create output folder if not exists
os.makedirs(extract_path, exist_ok=True)

# Unzip
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("Files extracted to:", extract_path)

import os
import pandas as pd
import spacy
from collections import Counter
import subprocess
from docx import Document
from docx.opc.exceptions import PackageNotFoundError
import PyPDF2

#os finds the files, docx reads the content from them, and pandas puts it all together into a useful table

# Read .doc file
def read_doc(file_path):
    """Read .doc files using antiword system tool"""
    try:
        result = subprocess.run(['antiword', file_path], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)
        if result.returncode == 0:
            return result.stdout
        else:
            print(f"Error reading {file_path}: {result.stderr}")
            return ""
    except FileNotFoundError:
        print("antiword not found. Please make sure it is installed.")
        return ""
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return ""

# Read .docx file
def read_docx(file_path):
    try:
        doc = Document(file_path)
        return '\n'.join([para.text for para in doc.paragraphs])
    except PackageNotFoundError:
        print(f"Skipping invalid/corrupted .docx file: {file_path}")
        return ""
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return ""

# Read .pdf file
def read_pdf(file_path):
    """Read .pdf files using PyPDF2"""
    try:
        with open(file_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            text = []
            for page in reader.pages:
                text.append(page.extract_text())
            return '\n'.join(text)
    except Exception as e:
        print(f"Error reading {file_path}: {e}")
        return ""

data = []
# Replace with the path to your folder in Google Drive
root_dir = "/content/drive/MyDrive/resume_dataset_unzipped"

if not os.path.exists(root_dir):
    print(f"Error: The directory '{root_dir}' does not exist.")
else:
    for subdir, dirs, files in os.walk(root_dir):
        role = os.path.basename(subdir)

        if role == os.path.basename(root_dir):
            continue

        for file in files:
            file_path = os.path.join(subdir, file)
            ext = file.lower().split('.')[-1]
            text = ""

            # Explicitly check for the problem file and use the .doc reader
            if "Peoplesoft Admin_G Ananda Rayudu" in file:
                print(f"Forcing .doc reader for: {file}")
                text = read_doc(file_path)
            elif ext == 'doc':
                text = read_doc(file_path)
            elif ext == 'docx':
                text = read_docx(file_path)
            elif ext == 'pdf':
                text = read_pdf(file_path)
            else:
                print(f"Skipping unsupported file type: {file_path}")

            if text.strip() != "":
                data.append({'text': text, 'role': role})

# Convert the list of dictionaries to a DataFrame
df = pd.DataFrame(data)

print("\nFinal DataFrame:")
print(df)

!ls "/content/drive/MyDrive/resume_dataset_unzipped"

print("\nFinal DataFrame:")
print(df)

print(df.head())

import re
import nltk
from nltk.corpus import stopwords
import pandas as pd

try:
    nltk.download('stopwords', quiet=True)
except Exception:
    print("NLTK stopwords download failed. Please check your connection.")

df_cleaned = df.copy()

df_cleaned.dropna(inplace=True)
print("Removed rows with missing values.")

def clean_text(text):
    # Regex for emails
    text = re.sub(r'\S*@\S*\s?', '', text)
    # Regex for URLs
    text = re.sub(r'http\S+|www.\S+', '', text)
    # Regex for US phone numbers (can be adjusted for other formats)
    text = re.sub(r'(\+?\d{1,2})?[\s.-]?\(?\d{3}\)?[\s.-]?\d{3}[\s.-]?\d{4}', '', text)
    return text

df_cleaned['text'] = df_cleaned['text'].apply(clean_text)
print("Removed emails, URLs, and phone numbers.")

stop_words = set(stopwords.words('english'))

def remove_stopwords(text):
    words = text.split()
    filtered_words = [word for word in words if word.lower() not in stop_words]
    return " ".join(filtered_words)

df_cleaned['text'] = df_cleaned['text'].apply(remove_stopwords)
print("Removed stop words.")

print("\n--- Cleaned DataFrame ---")
pd.set_option('display.max_colwidth', 500) # To see more of the text content
print(df_cleaned.head())

# Load the spaCy English language model
try:
    nlp = spacy.load("en_core_web_sm")
    print("spaCy model loaded successfully.")
except Exception as e:
    print(f"Error loading spaCy model: {e}")
    print("Please make sure you have run the installation and download commands.")

df_processed = df_cleaned.copy()

def final_clean_text(text):
    # Remove all punctuation and special characters
    text = re.sub(r'[^\w\s]', '', text)
    # Remove extra whitespace
    text = re.sub(r'\s+', ' ', text).strip()
    return text

df_processed['content_final'] = df_processed['text'].apply(final_clean_text)
print("Punctuation and extra whitespace removed.")

def count_numbers(text):
    # Regex to find any sequence of digits
    return len(re.findall(r'\d+', text))

df_processed['number_count'] = df_processed['content_final'].apply(count_numbers)
print("Number count added to DataFrame.")

# Function to get named entities from text
def get_named_entities(text):
    doc = nlp(text)
    entities = [ent.label_ for ent in doc.ents]
    return entities

# This may take a few minutes depending on the size of your dataset
df_processed['named_entities'] = df_processed['content_final'].apply(get_named_entities)

# Get a flattened list of all entities
all_entities = [entity for sublist in df_processed['named_entities'] for entity in sublist]

# Count the most common entities
top_entities = Counter(all_entities).most_common(15)

print("\n--- Top Named Entities ---")
print(top_entities)

# Display the first few rows of the updated DataFrame
print("\n--- DataFrame with New Columns ---")
pd.set_option('display.max_columns', None)
pd.set_option('display.max_colwidth', 500)
print(df_processed[['role', 'content_final']])

df_entities = df_cleaned.copy()

def extract_filtered_entities(text):
    doc = nlp(text)
    # Filter for interesting entity types like PERSON, ORG, GPE
    # We convert to lowercase to handle variations (e.g., 'Google' vs 'google')
    entities = [ent.text.lower() for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE']]
    return entities

print("Extracting entities. This may take a moment...")
df_entities['extracted_entities'] = df_entities['text'].apply(extract_filtered_entities)

all_entities = [entity for sublist in df_entities['extracted_entities'] for entity in sublist]

common_names_to_exclude = ['experience', 'inc', 'university', 'llc', 'corporation', 'b.e.']
all_entities = [entity for entity in all_entities if entity not in common_names_to_exclude]

top_entities = Counter(all_entities).most_common(20)

print("\n--- Top 20 Most Common Specific Entities ---")
print(top_entities)

import matplotlib.pyplot as plt

labels, values = zip(*top_entities)
plt.figure(figsize=(15, 8))
plt.bar(labels, values, color='teal')
plt.title('Top 20 Most Common Entities (People, Organizations, Locations)')
plt.xlabel('Entity')
plt.ylabel('Count')
plt.xticks(rotation=60, ha='right')
plt.tight_layout()
plt.show()

plt.figure(figsize=(10, 6))
plt.hist(df_processed['number_count'], bins=30, color='lightgreen', edgecolor='black')
plt.title('Distribution of Number Counts Per Resume')
plt.xlabel('Number of Digits/Sequences in Resume')
plt.ylabel('Frequency')
plt.grid(axis='y', alpha=0.75)
plt.show()

import matplotlib.pyplot as plt

category_counts = df_cleaned['role'].value_counts()

plt.figure(figsize=(10, 6))
category_counts.plot(kind='bar', color='skyblue')
plt.title('Distribution of Resumes by Category')
plt.xlabel('Category')
plt.ylabel('Number of Resumes')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

category_counts = df_cleaned['role'].value_counts()

plt.figure(figsize=(8, 8))
plt.pie(category_counts, labels=category_counts.index, autopct='%1.1f%%', startangle=140, colors=plt.cm.Paired.colors)
plt.title('Percentage of Resumes by Category')
plt.axis('equal') # Ensures the pie chart is circular.
plt.show()

import seaborn as sns

plt.figure(figsize=(10, 6))
sns.countplot(x='role', data=df_cleaned, order=df['role'].value_counts().index)
plt.title('Number of Resumes in Each Category')
plt.xlabel('Category')
plt.ylabel('Number of Resumes')
plt.xticks(rotation=45, ha='right')
plt.tight_layout()
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix

df = df_processed[['content_final', 'role']].copy()

print("--- Preparing the Data ---")
# 1. Split the data into features (X) and target (y)
# X is the text content, and y is the category/role
X = df['content_final']
y = df['role']

# Assuming you have your raw text data in a variable called `text_data`
# and the corresponding labels in a variable called `labels`.

from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression

# Step 1: Split your original data into training and testing sets.
# This is the most crucial step to ensure X and y remain aligned.
# Assuming you have a DataFrame named 'df' with 'content' and 'role' columns
# Let's say you've already performed some preprocessing on the content
# and stored it in a new variable.

# This is the step that was likely missing
content_final = df['content_final']
role = df['role']

# Now the train_test_split function will work because both variables are defined
X_train, X_test, y_train, y_test = train_test_split(content_final, role, test_size=0.3, random_state=57)


# Step 2: Initialize the TF-IDF vectorizer.
vectorizer = TfidfVectorizer()

# Step 3: Fit the vectorizer on the training data and transform it.
# This creates a feature matrix with the same number of rows as X_train.
X_train_tfidf = vectorizer.fit_transform(X_train)

# Step 4: Initialize and train your logistic regression model.
# Now, X_train_tfidf and y_train have the same number of samples, so the fit() method will work.
log_reg_model = LogisticRegression()
log_reg_model.fit(X_train_tfidf, y_train)

# Optional: Transform the test data to evaluate the model
X_test_tfidf = vectorizer.transform(X_test)

print(f"Shape of X_train_tfidf: {X_train_tfidf.shape}")
print(f"Shape of y_train: {y_train.shape}")

print("\n--- Training and Evaluating Models ---")

# --- Model 1: Logistic Regression ---
# Logistic Regression is a simple, fast, and highly interpretable linear model.
# It's an excellent starting point and a good baseline to compare more complex models against.
print("\n--- Training Logistic Regression Model ---")
#This is a simple, linear model that serves as an excellent starting point.
#It's fast to train and easy to interpret, making it a great baseline for comparison.
#It assumes a linear relationship between your words and the resume category.

log_reg_model = LogisticRegression(max_iter=1000)

log_reg_model.fit(X_train_tfidf, y_train)

y_pred_log_reg = log_reg_model.predict(X_test_tfidf)

print("\n--- Evaluation: Logistic Regression ---")
print("Classification Report:")
print(classification_report(y_test, y_pred_log_reg))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_log_reg))

# --- Model 2: Support Vector Machine (SVC) ---
# An SVC finds the best hyperplane to separate different classes.
# It is very powerful and often performs well on text classification tasks.
print("\n--- Training Support Vector Machine (SVC) Model ---")
#A more powerful and complex model that finds the best hyperplane to separate your resume data points into different categories.
#It is highly effective in high-dimensional spaces, which is perfect for text data.

svm_model = SVC(kernel='linear') # 'linear' kernel is common for text data

svm_model.fit(X_train_tfidf, y_train)

y_pred_svm = svm_model.predict(X_test_tfidf)

print("\n--- Evaluation: Support Vector Machine ---")
print("Classification Report:")
print(classification_report(y_test, y_pred_svm))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_svm))

# --- Model 3: Random Forest Classifier ---
# A Random Forest is an ensemble method that builds multiple decision trees.
# It is very robust and can handle complex, non-linear relationships in the data.
print("\n--- Training Random Forest Model ---")
#An ensemble model that builds multiple decision trees. It is highly robust to noisy data and is effective at capturing complex, non-linear patterns.
# It can provide a more powerful and often more accurate solution than a single linear model.

rf_model = RandomForestClassifier(n_estimators=100, random_state=42)

rf_model.fit(X_train_tfidf, y_train)

y_pred_rf = rf_model.predict(X_test_tfidf)

print("\n--- Evaluation: Random Forest Classifier ---")
print("Classification Report:")
print(classification_report(y_test, y_pred_rf))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_rf))

from sklearn.naive_bayes import MultinomialNB
import xgboost as xgb

# 3. Filter out empty strings to create the 'filtered' variables
# This is the step that creates the variable y_train_filtered
X_train_filtered = X_train[X_train.str.strip().astype(bool)]
y_train_filtered = y_train[X_train_filtered.index]

print("\n--- Training Naive Bayes Classifier Model ---")
# Naive Bayes is a simple, probabilistic classifier that is great for text data.
# It is very fast and efficient.
nb_model = MultinomialNB()

nb_model.fit(X_train_tfidf, y_train_filtered)

y_pred_nb = nb_model.predict(X_test_tfidf)

print("\n--- Evaluation: Naive Bayes Classifier ---")
print("Classification Report:")
print(classification_report(y_test, y_pred_nb))
print("Confusion Matrix:")

print(confusion_matrix(y_test, y_pred_nb))

from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.metrics import classification_report
import numpy as np
from sklearn.model_selection import train_test_split, GridSearchCV

print("--- Building the Pipeline ---")
# 1. Split the data into features (X) and target (y)
X = df_processed['content_final']
y = df_processed['role']

# 2. Encode the string labels to numerical labels
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

# 3. Split the data into training and testing sets using the encoded labels
X_train, X_test, y_train, y_test = train_test_split(X, y_encoded, test_size=0.3, random_state=42)

print(f"Training set size: {X_train.shape[0]} samples")
print(f"Testing set size: {X_test.shape[0]} samples")

# 4. Create the Pipeline for Logistic Regression
# We'll use a simpler pipeline for a cleaner demonstration
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=2000)),
    ('log_reg', LogisticRegression(random_state=10, max_iter=1000))
])

# --- Section for Parameter Tuning ---
print("\n--- Hyperparameter Tuning using Grid Search ---")

# Define the parameter grid to search
param_grid = {
    'tfidf__max_features': [2000, 5000, 10000],
    'log_reg__C': [0.1, 1, 10, 100]
}

# Create a GridSearchCV object
grid_search = GridSearchCV(pipeline, param_grid, cv=3, verbose=1, n_jobs=-1)
grid_search.fit(X_train, y_train)

# Get the best parameters and best score
print(f"Best parameters found: {grid_search.best_params_}")
print(f"Best cross-validation score: {grid_search.best_score_:.4f}")

best_pipeline = grid_search.best_estimator_

# 5. Train the best pipeline on the entire training data (already done by GridSearchCV)
print("\n--- Training the Pipeline ---")
best_pipeline.fit(X_train, y_train)

# 6. Make predictions on the test data
y_pred = best_pipeline.predict(X_test)
y_pred

# 7. Evaluate the pipeline's performance
print("\n--- Evaluating Pipeline Performance ---")
print("Classification Report:")
print(classification_report(y_test, y_pred, labels=np.unique(y_test), target_names=label_encoder.classes_))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))

import joblib
import pickle

# --- Model Deployment: Saving the Pipeline and Label Encoder ---
print("\n--- Saving the Trained Model and Label Encoder ---")

pipeline_filename = 'resume_classifier_log_reg_pipeline.pkl'
with open(pipeline_filename, 'wb') as file:
    pickle.dump(best_pipeline, file)
print(f"Pipeline saved to '{pipeline_filename}'")

label_encoder_filename = 'label_encoder_log_reg.joblib'
joblib.dump(label_encoder, label_encoder_filename)
print(f"Label encoder saved to '{label_encoder_filename}'")

# --- Example of Loading and Using the Saved Model for Deployment ---
print("\n--- Loading the Saved Model for Prediction ---")

loaded_pipeline = joblib.load(pipeline_filename)
loaded_label_encoder = joblib.load(label_encoder_filename)

new_resume_text = "I have experience in database management, SQL queries, and data warehousing."

prediction_encoded = loaded_pipeline.predict([new_resume_text])
predicted_role = loaded_label_encoder.inverse_transform(prediction_encoded)[0]

"""# **Streamlit Deployment**

"""

!pip install streamlit

import streamlit as st
import joblib
import pickle
import warnings
from docx import Document
from PyPDF2 import PdfReader

warnings.filterwarnings('ignore')

# --- Load the saved pipeline and label encoder ---

@st.cache_resource
def load_model():
    """
    Loads the trained model pipeline and label encoder from the saved files.
    This function is cached to prevent reloading the model on every user interaction.
    """
    try:
        # Load the pipeline with pickle
        with open('resume_classifier_log_reg_pipeline.pkl', 'rb') as file:
            loaded_pipeline = pickle.load(file)

        # Load the label encoder with joblib
        loaded_label_encoder = joblib.load('label_encoder_log_reg.joblib')

        return loaded_pipeline, loaded_label_encoder
    except FileNotFoundError:
        st.error("Error: The model files were not found. Please ensure 'resume_classifier_log_reg_pipeline.pkl' and 'label_encoder_log_reg.joblib' are in the same directory.")
        return None, None

pipeline, label_encoder = load_model()

# --- Helper function to read file content ---
def get_text_from_file(file):
    """
    Extracts text from different file types.
    """
    file_extension = file.name.split('.')[-1].lower()

    if file_extension == 'docx':
        doc = Document(file)
        full_text = []
        for para in doc.paragraphs:
            full_text.append(para.text)
        return '\n'.join(full_text)

    elif file_extension == 'pdf':
        reader = PdfReader(file)
        text = ""
        for page in reader.pages:
            text += page.extract_text()
        return text

    elif file_extension == 'txt':
        return file.read().decode('utf-8')

    return ""

# --- Create the Streamlit App UI ---
st.set_page_config(page_title="Resume Classifier", layout="centered")

st.title("Resume Classifier App")
st.markdown("Upload a resume file below to get the predicted job role.")
st.markdown("---")

if pipeline and label_encoder:
    # File uploader for user input
    uploaded_file = st.file_uploader(
        "Upload a Resume File",
        type=['docx', 'pdf', 'txt'],
        help="Supported formats: .docx, .pdf, .txt"
    )

    if st.button("Predict Role"):
        if uploaded_file is not None:
            try:
                # Get text from the uploaded file
                resume_text = get_text_from_file(uploaded_file)

                if resume_text.strip(): # Check if text is not empty
                    # Use the loaded pipeline to make a prediction
                    prediction_encoded = pipeline.predict([resume_text])

                    # Use the label encoder to get the human-readable class name
                    predicted_role = label_encoder.inverse_transform(prediction_encoded)[0]

                    st.success(f"**Predicted Job Role:** {predicted_role}")
                else:
                    st.warning("The uploaded file appears to be empty or unreadable. Please try a different file.")
            except Exception as e:
                st.error(f"An error occurred during prediction: {e}")
        else:
            st.warning("Please upload a file to get a prediction.")
else:
    st.error("The application could not load the necessary model files. Please check your file paths.")

